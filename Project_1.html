<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Real-Time Streaming Project Overview</title>
  <link rel="stylesheet" href="styles.css"> <!-- Link to your CSS file -->
  <style>
    body {
      background-color: #1e1e1e;
      color: #d4d4d4;
      font-family: Arial, sans-serif;
    }

    .container {
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }

    h2 {
      color: #4ecdc4;
      font-size: 2rem;
      margin-bottom: 20px;
      text-align: center;
    }

    h3, h4, h5 {
      color: #4ecdc4;
    }

    p, li {
      font-size: 1.1rem;
      line-height: 1.6;
    }

    a {
      color: #4ecdc4;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    img {
      border-radius: 10px;
      box-shadow: 0 4px 10px rgba(0, 0, 0, 0.5);
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }

    table, th, td {
      border: 1px solid #4ecdc4;
    }

    th, td {
      padding: 12px;
      text-align: left;
    }

    th {
      background-color: #4ecdc4;
      color: #1e1e1e;
    }

    pre {
      background-color: #2d2d2d;
      padding: 15px;
      border-radius: 5px;
      color: #d4d4d4;
      overflow-x: auto;
    }
  </style>
</head>
<body>
  <div class="container">
    <h2>Real-Time Streaming with Azure Databricks and Event Hubs</h2>
    <img src="assets/streaming_project1.png" alt="Project Architecture" style="width: 100%; height: auto; margin-bottom: 40px;">

    <h3>Project Overview</h3>
    <p>This project showcases a comprehensive real-time data streaming solution utilizing Azure Event Hubs, Databricks with Spark Structured Streaming, Delta Lake, and Power BI for visualization. It implements the Medallion architecture, processing data through Bronze, Silver, and Gold layers for efficient data management and analysis..</p>

    <h4>Architecture Stages</h4>
    <ul>
      <li><strong>Ingest:</strong> Streaming data from various sources such as Kafka, IoT devices, and log data is ingested into Azure Event Hubs.</li>
      <li><strong>Process:</strong> The data is processed using Databricks with Spark Structured Streaming. The data is stored in Delta Lake across Bronze, Silver, and Gold layers.</li>
      <li><strong>Serve:</strong> The processed data in the Gold layer is made available for reporting in Power BI.</li>
    </ul>

    <h4>Medallion Architecture</h4>
    <ul>
        <h2>Medallion Architecture Overview</h2>
<img src="assets/medallion_architecture.jpg" alt="Medallion Architecture Overview" style="width:100%; max-width:600px;">

<ul>
  <li><strong>Bronze Layer:</strong> Stores raw, unprocessed data. Minimal transformations are applied to preserve the original data. This layer acts as the raw data archive and is critical for auditing and compliance.</li>
  <li><strong>Silver Layer:</strong> Data is cleaned, filtered, and enriched. It is transformed into a more consumable format, typically denormalized and optimized for querying and analysis. This layer is often used for intermediate reporting and machine learning model training.</li>
  <li><strong>Gold Layer:</strong> Aggregated and refined data is stored in this layer. This data is curated for specific business needs and is highly optimized for performance. It is often used for business intelligence tools like Power BI, providing a single source of truth for dashboards and reports.</li>
</ul>

<h3>Azure Event Hubs</h3>
<img src="assets/event_hub.jfif" alt="Azure Event Hub" style="width:100%; max-width:60px;">
<h5>Overview</h5>
<p>Azure Event Hubs is a big data streaming platform and event ingestion service capable of receiving and processing millions of events per second. It acts as a distributed data streaming platform similar to Apache Kafka and is designed to handle large volumes of data ingestion with low latency.</p>

<h4>Azure Event Hubs</h4>
<img src="assets/Event_hubs_data_flow.png" alt="How Azure Event Data Flow Works" style="width:100%; max-width:600px;">

<h5>Features</h5>
<ul>
 <li><strong>Real-time Data Ingestion:</strong> Event Hubs allows for the ingestion of data in real time with high throughput, making it ideal for applications like financial transactions, live gaming, social media analytics, and IoT data processing.</li>
  <li><strong>Partitioned Consumer Model:</strong> This model enables parallel processing of data, improving scalability and performance. Each partition is an independent sequence of data in the Event Hub.</li>
  <li><strong>Event Retention:</strong> You can configure the retention period for events to ensure that the data is available for the required duration. This is crucial for batch processing and reprocessing historical data.</li>
  <li><strong>Capture:</strong> Automatically capture streaming data into Azure Blob storage or Azure Data Lake Store for long-term retention, archiving, or further batch processing. This feature simplifies the integration with data lakes.</li>
  <li><strong>Seamless Integration:</strong> Event Hubs integrates seamlessly with Azure Stream Analytics, Azure Functions, and Azure Logic Apps, providing multiple options for processing data in motion.</li>
</ul>

<h4>Test Data Generation</h4>
<p>In this project, we're generating fake weather data in JSON format. The data includes attributes such as temperature, humidity, wind speed, wind direction, and precipitation. This approach helps in testing the data pipeline with various edge cases, ensuring robustness and reliability. Below is an example of the JSON format used:</p>
<pre>
{
  "temperature": 20,
  "humidity": 60,
  "windSpeed": 10,
  "windDirection": "NW",
  "precipitation": 0,
  "conditions": "Partly Cloudy"
}
</pre>

<h4>Tools Used</h4>
<ul>
  <li><strong>Azure Event Hubs:</strong> For ingesting streaming data from multiple sources in real time.</li>
  <li><strong>Databricks:</strong> For processing the data using Spark Structured Streaming, leveraging its distributed computing capabilities to handle large volumes of streaming data efficiently.</li>
  <li><strong>Delta Lake:</strong> For storing data in a structured format across Bronze, Silver, and Gold layers. Delta Lake provides ACID transactions and scalable metadata handling for lakehouse architecture.</li>
  <li><strong>Azure Data Lake Storage (ADLS):</strong> For long-term storage of data, allowing for scalable and cost-effective storage of large datasets.</li>
  <li><strong>Unity Catalog:</strong> For managing metadata and ensuring data governance, helping in maintaining security, and data quality across the organization.</li>
  <li><strong>Power BI:</strong> For creating interactive reports and visualizing the processed data, enabling stakeholders to make data-driven decisions.</li>
  <li><strong>Azure Synapse Analytics:</strong> For running complex analytical queries on big data sets and integrating data with other Azure services for advanced analytics.</li>
</ul>

<h4>Project Plan</h4>
<ul>
  <li><strong>Data Ingestion:</strong> Set up Azure Event Hubs to receive streaming data. Create a Databricks workspace and connect it to Azure Event Hubs. Utilize Azure Synapse or Azure Data Factory for orchestrating data movement and transformation.</li>
  <li><strong>Data Processing:</strong> Implement Spark Structured Streaming in Databricks to process the data in near real-time. Store the raw data in the Bronze layer. Apply data quality checks and transformations to create Silver data. Aggregate, refine, and apply business logic to the data to store it in the Gold layer.</li>
  <li><strong>Data Serving:</strong> Connect Power BI to the Gold layer in Delta Lake. Create a Power BI report to visualize the weather data, enabling stakeholders to access real-time insights.</li>
  <li><strong>Testing:</strong> Validate the data processing pipeline by generating test data. Ensure the data flows correctly from Azure Event Hubs through to Power BI. Perform unit testing and end-to-end testing to ensure data accuracy and pipeline resilience.</li>
  <li><strong>Deployment:</strong> Deploy the solution in a production environment. Monitor the data pipeline using Azure Monitor, Databricks Jobs, and Event Hub metrics. Adjust configurations as needed for performance and scalability. Implement disaster recovery and failover strategies.</li>
  <li><strong>Maintenance:</strong> Regularly update the pipeline components, optimize Spark jobs, and ensure compliance with data governance policies. Implement automated monitoring and alerting to identify and resolve issues proactively.</li>
</ul>

<h4>Notes on Real-Time Data Processing with Azure Databricks and Event Hubs</h4>
<h5>Output Modes in Spark Structured Streaming</h5>
<table>
  <thead>
    <tr>
      <th>Output Mode</th>
      <th>Behaviour</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Complete</td>
      <td>The entire updated result table is written to external storage. This mode is useful when you need to keep an accurate snapshot of the data at each interval but can be resource-intensive for large datasets.</td>
    </tr>
    <tr>
      <td>Append</td>
      <td>Only new rows appended in the result table since the last trigger are written to external storage. This mode is efficient for streaming data where you only need the latest data, such as log files.</td>
    </tr>
    <tr>
      <td>Update</td>
      <td>Only the rows that were updated in the result table since the last trigger are written to external storage. If the query doesn't contain aggregations, it is equivalent to Append mode. This mode is suitable for data that changes frequently and requires updates to be reflected promptly.</td>
    </tr>
  </tbody>
</table>

<h5>Checkpointing in Spark Structured Streaming</h5>
<p>Checkpointing is used to prevent duplicate outputs during writeStream operations, even after job restarts or failures. It ensures exactly-once semantics in Spark streaming, which is crucial for maintaining data integrity and consistency across distributed environments.</p>

<h5>Windowing in Spark Structured Streaming</h5>
<p>Windowing Behavior: The latest window will close when an event with an event time later than 3:25 is received. This is because the upper bound of the window is 3:20. So, an event time after 3:25 is greater than the upper bound plus 5 minutes. This behavior is essential for time-series analysis and aggregations.</p>

<h4>Power BI Integration</h4>
<p>To connect with Power BI, you can use Partner Connect for seamless integration between Databricks and Power BI. This allows for real-time updates of the Power BI dashboards, ensuring that the insights are always up-to-date with the latest data.</p>

<h4>Security and Compliance</h4>
<p>Ensure that all data processing and storage comply with organizational and regulatory requirements. Use Azure Active Directory for identity management, and implement role-based access control (RBAC) for securing resources. Encrypt data at rest and in transit using Azure Key Vault for managing encryption keys.</p>

<a href="https://github.com/Akshay107appa/Real-Time-Streaming-with-Azure-Databricks-and-Event-Hubs.git" style="display: inline-block; padding: 10px 20px; background-color: #333; color: #fff; text-align: center; text-decoration: none; border-radius: 5px;">
    <img src="assets/GitHub_Logo.png" alt="GitHub Logo" style="height: 40px; vertical-align: middle; margin-right: 80px;">
    View the full project on GitHub
</a>


<a href="https://github.com/Akshay107appa/Real-Time-Streaming-with-Azure-Databricks-and-Event-Hubs.git" style="display: inline-block; padding: 10px 20px; background-color: #333; color: #fff; text-align: center; text-decoration: none; border-radius: 5px;">
    <img src="assets/GitHub_Logo.png" alt="GitHub Logo" style="height: 40px; vertical-align: middle; margin-right: 80px;">
    View the full project on GitHub
</a>
